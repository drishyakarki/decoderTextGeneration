# GPT-2-based Text Generation

## Introduction
Continuing my NLP journey, I've been following HuggingFace's NLP course, delving into Transformer models and their applications. Next fundamental concept is Decoder, which play a crucial role in NLP tasks. While Encoders and Decoders are often used together in sequence-to-sequence models for generative tasks, decoders-only can also be used independently, usually good for generative tasks such as text generation.

In this simple project, I implemented the GPT-2 model for text generation.

## Project Highlights
To deepen my understanding, I learned about the following steps in this project:

### 1. Import GPT2LMHeadModel for Text generation and GPT2Tokenizer for tokenizing the text.

### 2. Load the model.

### 3. Tokenize the input text

### 4. Finally generate the text.

## Getting Started
To run this project on your own, follow these steps:

1. Clone the repository.
2. Create a virtual environment using `python -m venv venv`.
3. Install the necessary dependencies using `pip install -r requirements.txt`.

Feel free to explore the code and adapt it to your own projects. Enjoy your NLP journey!

## Creator
Drishya Karki
